{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "The `torch.nn.Embedding` layer maps indices to dense vectors. It’s useful when you want to get a vector representation for each discrete input (e.g., a word or token)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- `num_embeddings=3`: The number of unique tokens in the vocabulary (3 in this case).\n",
    "- `embedding_dim=5`: The size of the embedding vector for each token.\n",
    "The output will be a dense matrix of shape [3, 5] (3 tokens, each represented by a 5-dimensional vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Output:\n",
      "tensor([[-0.3792,  0.4097,  0.5401, -0.8731,  1.1601],\n",
      "        [ 0.3585,  0.7492, -0.7993, -0.2771,  0.1574],\n",
      "        [ 0.5417, -0.5474, -0.3932,  0.0042,  0.6709]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the embedding layer\n",
    "embedding = nn.Embedding(num_embeddings=3, embedding_dim=5)\n",
    "\n",
    "# Input indices for the sentence \"I love AI\"\n",
    "input_indices = torch.tensor([0, 1, 2])  # [I, love, AI]\n",
    "\n",
    "# Get the embeddings\n",
    "output = embedding(input_indices)\n",
    "print(\"Embedding Output:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Output:\n",
      "tensor([[-0.3792,  0.4097,  0.5401, -0.8731,  1.1601],\n",
      "        [ 0.3585,  0.7492, -0.7993, -0.2771,  0.1574],\n",
      "        [ 0.5417, -0.5474, -0.3932,  0.0042,  0.6709]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the embedding layer\n",
    "embedding = nn.Embedding(num_embeddings=3, embedding_dim=5)\n",
    "\n",
    "# Input indices for the sentence \"I love AI\"\n",
    "input_indices = torch.tensor([0, 1, 2])  # [I, love, AI]\n",
    "\n",
    "# Get the embeddings\n",
    "output = embedding(input_indices)\n",
    "print(\"Embedding Output:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Bag\n",
    "`torch.nn.EmbeddingBag` computes embeddings for a batch of sequences but applies pooling (e.g., summing, averaging) to combine embeddings for sequences. It’s efficient for tasks where you only need aggregated representations, like sentence-level embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingBag Output:\n",
      "tensor([[ 0.9563,  0.8776, -0.3627,  0.7169,  0.7994],\n",
      "        [ 0.1821, -0.5768,  0.4983, -0.3879, -0.7234]],\n",
      "       grad_fn=<EmbeddingBagBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define the embedding bag layer\n",
    "embedding_bag = nn.EmbeddingBag(num_embeddings=5, embedding_dim=5, mode='mean')\n",
    "\n",
    "# Input indices for the sentences\n",
    "input_indices = torch.tensor([0, 1, 2, 2, 3, 4])  # [I, love, AI, AI, is, great]\n",
    "\n",
    "# Offsets to define where each sentence starts\n",
    "offsets = torch.tensor([0, 3])  # Sentence 1 starts at index 0, Sentence 2 at index 3\n",
    "\n",
    "# Get the aggregated embeddings\n",
    "output = embedding_bag(input_indices, offsets)\n",
    "print(\"EmbeddingBag Output:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Embedding**: The output for the individual words \"I\", \"like\", and \"cats\" will be three separate vectors.\n",
    "- **Embedding Bag**: The output will be a single vector that represents the average of the embeddings for \"I\", \"like\", and \"cats\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
